name: Check ML Runner Readiness

on:
  workflow_dispatch:

jobs:
  check-gpu:
    name: GPU & CUDA Check
    runs-on: self-hosted

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: System Information
        run: |
          echo "=== OS Information ==="
          uname -a
          cat /etc/os-release
          echo ""
          echo "=== CPU Information ==="
          lscpu | grep -E "Model name|Socket|Core|Thread"
          echo ""
          echo "=== Memory Information ==="
          free -h

      - name: Check NVIDIA GPU
        run: |
          echo "=== GPU Detection ==="
          if command -v nvidia-smi &> /dev/null; then
            nvidia-smi
          else
            echo "nvidia-smi not found"
            exit 1
          fi

      - name: Check CUDA Installation
        run: |
          echo "=== CUDA Version ==="
          if command -v nvcc &> /dev/null; then
            nvcc --version
          else
            echo "nvcc not in PATH, checking /usr/local/cuda..."
            if [ -f /usr/local/cuda/bin/nvcc ]; then
              /usr/local/cuda/bin/nvcc --version
            else
              echo "CUDA toolkit not found"
              exit 1
            fi
          fi

      - name: Check CUDA Libraries
        run: |
          echo "=== CUDA Libraries ==="
          ldconfig -p | grep -E "libcuda|libcudart|libnvrtc" || echo "Some CUDA libraries may not be in ldconfig"
          echo ""
          echo "=== CUDA Installation Path ==="
          ls -la /usr/local/cuda*/lib64/*.so.* 2>/dev/null | head -10 || echo "Checking alternative paths..."
          ls -la /usr/lib/x86_64-linux-gnu/libcuda* 2>/dev/null | head -5 || true

  check-python-ml:
    name: Python & ML Libraries Check
    runs-on: self-hosted
    needs: check-gpu

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Check Python
        run: |
          echo "=== Python Version ==="
          python3 --version
          echo ""
          echo "=== Pip Version ==="
          pip3 --version || python3 -m pip --version

      - name: Check PyTorch GPU Support
        run: |
          echo "=== PyTorch GPU Check ==="
          python3 << 'EOF'
          try:
              import torch
              print(f"PyTorch version: {torch.__version__}")
              print(f"CUDA available: {torch.cuda.is_available()}")
              if torch.cuda.is_available():
                  print(f"CUDA version: {torch.version.cuda}")
                  print(f"GPU count: {torch.cuda.device_count()}")
                  for i in range(torch.cuda.device_count()):
                      print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
                      props = torch.cuda.get_device_properties(i)
                      print(f"    Memory: {props.total_memory / 1024**3:.1f} GB")
                      print(f"    Compute capability: {props.major}.{props.minor}")
              else:
                  print("CUDA not available in PyTorch")
                  exit(1)
          except ImportError:
              print("PyTorch not installed")
              exit(1)
          EOF

      - name: Check TensorFlow GPU Support
        continue-on-error: true
        run: |
          echo "=== TensorFlow GPU Check ==="
          python3 << 'EOF'
          try:
              import tensorflow as tf
              print(f"TensorFlow version: {tf.__version__}")
              gpus = tf.config.list_physical_devices('GPU')
              print(f"GPU devices: {len(gpus)}")
              for gpu in gpus:
                  print(f"  {gpu}")
              if not gpus:
                  print("No GPU detected by TensorFlow")
          except ImportError:
              print("TensorFlow not installed")
          EOF

      - name: Check Other ML Libraries
        continue-on-error: true
        run: |
          echo "=== ML Libraries Check ==="
          python3 << 'EOF'
          libs = ['numpy', 'pandas', 'scikit-learn', 'matplotlib', 'jupyter']
          for lib in libs:
              try:
                  module = __import__(lib.replace('-', '_'))
                  version = getattr(module, '__version__', 'unknown')
                  print(f"{lib}: {version}")
              except ImportError:
                  print(f"{lib}: not installed")
          EOF

  gpu-compute-test:
    name: GPU Compute Test
    runs-on: self-hosted
    needs: check-python-ml

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Simple GPU Compute Test
        run: |
          echo "=== GPU Compute Test ==="
          python3 << 'EOF'
          import torch
          import time

          if not torch.cuda.is_available():
              print("CUDA not available")
              exit(1)

          device = torch.device("cuda")
          print(f"Using device: {torch.cuda.get_device_name(0)}")

          # Matrix multiplication benchmark
          size = 4096
          print(f"\nRunning {size}x{size} matrix multiplication...")

          a = torch.randn(size, size, device=device)
          b = torch.randn(size, size, device=device)

          # Warmup
          torch.cuda.synchronize()
          c = torch.matmul(a, b)
          torch.cuda.synchronize()

          # Benchmark
          start = time.time()
          for _ in range(10):
              c = torch.matmul(a, b)
          torch.cuda.synchronize()
          elapsed = time.time() - start

          tflops = (2 * size**3 * 10) / elapsed / 1e12
          print(f"Time for 10 iterations: {elapsed:.3f}s")
          print(f"Approximate performance: {tflops:.2f} TFLOPS")
          print("\nGPU compute test passed!")
          EOF

      - name: GPU Memory Test
        run: |
          echo "=== GPU Memory Test ==="
          python3 << 'EOF'
          import torch

          if not torch.cuda.is_available():
              exit(1)

          device = torch.device("cuda")
          props = torch.cuda.get_device_properties(0)
          total_mem = props.total_memory / 1024**3

          print(f"Total GPU memory: {total_mem:.1f} GB")
          print(f"Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
          print(f"Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB")

          # Try to allocate ~50% of GPU memory
          try:
              target_gb = total_mem * 0.5
              elements = int(target_gb * 1024**3 / 4)  # float32 = 4 bytes
              tensor = torch.randn(elements, device=device)
              print(f"\nSuccessfully allocated {target_gb:.1f} GB tensor")
              del tensor
              torch.cuda.empty_cache()
          except RuntimeError as e:
              print(f"Could not allocate large tensor: {e}")

          print("\nGPU memory test passed!")
          EOF

  summary:
    name: Summary
    runs-on: self-hosted
    needs: [check-gpu, check-python-ml, gpu-compute-test]
    if: always()

    steps:
      - name: Print Summary
        run: |
          echo "==========================================="
          echo "   AI/ML Runner Readiness Check Complete   "
          echo "==========================================="
          echo ""
          echo "Runner: ${{ runner.name }}"
          echo "OS: ${{ runner.os }}"
          echo ""
          echo "Job Results:"
          echo "  GPU & CUDA Check:      ${{ needs.check-gpu.result }}"
          echo "  Python & ML Libraries: ${{ needs.check-python-ml.result }}"
          echo "  GPU Compute Test:      ${{ needs.gpu-compute-test.result }}"
          echo ""
          if [ "${{ needs.gpu-compute-test.result }}" == "success" ]; then
            echo "Runner is ready for AI/ML workloads!"
          else
            echo "Runner has issues - check job logs above"
          fi
